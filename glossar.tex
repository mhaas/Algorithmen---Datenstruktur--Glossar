%% LyX 1.6.5 created this file.  For more info, see http://www.lyx.org/.
\documentclass[english]{scrreprt}
%\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{amsmath}
\usepackage{babel}

\begin{document}

\section*{Begriffe}


\subsection*{Funktionen}
\begin{description}
\item [{monoton~steigend~(fallend)}] Sei f(x) eine Funktion in Abhängigkeit von $x$, dann ist die Funktion monoton steigen (fallend), wenn für steigendes $x$ die Werte nie echt kleiner (größer) werden.
\item [{streng~monoton~steigend~(fallend)}] Sei f(x) eine Funktion in Abhängigkeit von $x$, dann ist die Funktion monoton steigen (fallend), wenn für steigendes $x$ die Werte immer echt größer (kleiner) werden.
\end{description}

\section*{Rechenregeln}


\subsection*{Logarithmus}
\begin{description}
\item [{Logarithmus}] ist die Umkehrfunktion der Exponentialfunktion: 
\item [{$b^{\log_{b}(x)}=x$}] und $\log_{b}(b^{x})=x$

Der Logarithmus beantwortet Fragen wie: wenn ich mit einer Zahl $x$ starte, und dann in jedem Schritt $x=x/y$ setze, wie oft kann das gemacht werden, bis die resultierende Zahl $\leq z$ wird? Das kann so dargestellt werden: $x/(y^i) = z$. Dies ist natürlich gleich $z * y^i = x \leftrightarrow y^i = x/z$. Wir fragen nach der Zahl $i$, also müssen wir auf beiden Seiten logarithmieren (zum Beispiel zur Basis $2$, dargestellt durch $\log$): $i \log y = \log(x/z) \leftrightarrow i = \frac{\log x - \log z}{\log y}$. In Algorithmen und Datenstrukturen fragen wir besonders häufig danach, wie oft man eine Zahl $n$ durch $2$ teilen kann, bis sie $1$ wird. Also: $n/2^i = 1? \leftrightarrow \log n = i$. 
\item [{Produkte}] $\log_{a}(x\times y)=\log_{a}(x)+\log_{a}(y)$
\item [{Quotienten}] $\log_{a}(\frac{x}{y})=\log_{a}(x)-\log_{a}(y)$
\item [{Summen~und~Differenzen}] $\log_{a}(x+y)=\log_{a}(x)+\log_{a}(1+\frac{y}{x})$
\item [{Potenzen}] $log_{a}(x^{r})=r\times log_{a}(x)$
\item [{Basisumrechnung}] von Basis a zu Basis b: $\log_{b}(r)=\frac{\log_{a}(r)}{\log_{a}(b)}$
\end{description}

\section*{Formeln}


\subsection*{Landau-Symbole}

Landau-Symbole verwenden wir, um das asymptotische Verhalten einer
Laufzeitfunktion $f$ zu beschreiben.

Für $x\rightarrow\infty$:
\begin{description}
\item [{$f\in O(g)$}:] $\exists c>0\exists x_{0}\forall x>x_{0}:|f(x)|\leq c\times|g(x)|$
$\rightarrow$ f wächst höchstens so schnell wie g
\item [{$f\in\Omega(g)$}:] $\exists c>0\exists x_{0}\forall x>x_{0}:|f(x)|\ge c\times|g(x)|$
$\rightarrow$ f wächst mindestens so schnell wie g
\item [{$f\in\Theta(g)$}:] $\exists c_{0}>0\exists c_{1}>0\exists x_{0}\forall x>x_{0}:|c\times|g(x)|\leq|f(x)|\leq c_{1}\times|g(x)|$
$\rightarrow$ f wächst genauso schnell wie g
\end{description}

\subsubsection*{Achtung: das Gleichheitszeichen}

Oft wird das Gleichheitszeichen verwendet, um auszudrücken, dass eine
Funktion einer Klasse angehört. So wird auch in der Vorlesung beispielsweise$f(x)=O(x^{2})$
geschrieben, wenn es um eine Funktion quadratischer Laufzeit geht.
Tatsächlich ist das Gleichheitszeichen ein überladener Operator und
bezeichnet hier nicht die Gleichheit der beiden Ausdrücke, sondern
wird anstelle des $\in$-Symbols verwendet. $f(x)=O(x^{2})$ heißt
also nur, dass $f(x)$ in der Menge der Funktionen enthalten ist,
die durch $O(x^{2})$ beschrieben werden. Daher dürfen diese Gleichheitszeichen auch nur von links nach rechts gelesen werden.
$O(f(n)) = g(n)$ macht im Allgemeinen keinen Sinn! Außerdem sollten Sie sich diese Schreibweise wenn möglich nicht angewöhnen, sie wird nur leider in den gängigen Artikel und Textbüchern verwendet, so dass Sie sie kennen sollten.


\subsection*{Master-Theorem}

Wenn eine rekursive Gleichung der Laufzeitfunktion in Form $T(n)=\begin{cases}
a & \mbox{für n=1}\\
c\times n+d\times T(\frac{n}{b}) & \mbox{für n>1}\end{cases}$vorliegt, kann man mit Hilfe des Master-Theorems die Laufzeit bestimmen.

$T(n)=\begin{cases}
\Theta(n) & \mbox{wenn d<b}\\
\Theta(n\times\log(n)) & \mbox{wenn d=b}\\
O(n^{\log_{b}(d)}) & \mbox{wenn d>b}\end{cases}$

Die hier gezeigte Version des Master-Theorems ist beschränkt auf Algorithmen
mit linearen Nebenkosten (der Teil mit $c\times n$). In der Literatur
finden sich mächtigere Varianten.

\subsection*{Beweis durch Induktion}

Mit einem Beweis durch Induktion kann eine Aussage für alle
natürlichen Zahlen bewiesen werden.

In einem ersten Schritt wird der triviale Fall $n=1$
(oder auch $n=0$) bewiesen: dies ist unser Induktionsanfang.
Wir nehmen an, dass die zu beweisende Aussage für beliebige 
Zahlen $n$ gilt (Induktionsannahme).
Für den Beweis benötigen wir den Induktionsschritt: 
wir zeigen (durch Umformung),
dass die Aussage auch für $n+1$ gilt.
Ausgehend von unserem Basisfall $n=1$ wissen wir nun,
dass für alle folgenden Schritte die Aussage gültig ist.
Von $n=1$ kommen wir auf $n=2$ und wissen, dass die Aussage immer
noch gültig ist. Von $n=2$ auf $n=3$ etc.\\
\\
Ein einfaches Beispiel:\\
\\
Die zu beweisende Aussage (Multiplikation von n mit 3):\\
$\sum\limits_{1}^n{3}=3\times n$ \\ 
Induktionsanfang: $n=1$:\\
$\sum\limits_{1}^1{3}=3\times 1$ \\
Induktionsschritt mit $n+1$: \\
$\sum\limits_{1}^{n+1}{3}=(\sum\limits_{1}^n{3})+3=(3\times n) + 3 = 3\times (n+1)$\\
Der Trick in der Umformung ist, $\sum\limits_{1}^n{3}$ im zweiten Schritt
durch $3 \times n$ zu ersetzen. Das dürfen wir, da unsere Induktionsannahme
eben $\sum\limits_{1}^n{3}=3\times n$ lautet und wir annehmen, dass
diese Aussage bereits bewiesen ist.\\

Im übrigen funktioniert diese Beweismethode nicht nur für die natürlichen
Zahlen, sondern für jede fundierte Menge ($\rightarrow$ strukturelle Induktion). 
Es darf also keine unendlich
absteigenden Ketten in der natürlichen Ordnung geben, da es sonst
keinen Basisfall ($n=1$) gibt! Während für ganze negative Zahlen
ein Induktionsbeweis mit $n-1$ geführt werden kann, funktioniert
es mit der Menge der reellen Zahlen nicht mehr.




\end{document}
